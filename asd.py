# -*- coding: utf-8 -*-
"""ASD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SKQ70oU33HEkxwU2j3WPFNMdgAFnNm_x

Import libraries
"""

from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import RidgeClassifier

#import dataset
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("asd1.csv")

df.head()

df.isnull().sum()

df.describe()

print(df.columns)

print("Gender ",df['gender'].value_counts())

print("Ethnicity ",df['ethnicity'].value_counts())

print("Jaundice ",df['jaundice'].value_counts())

print("Autism", df['autism'].value_counts())

print("Country ",df['country_of_residence'].value_counts())

print("used_app_before", df['used_app_before'].value_counts())

print("Result",df['result'].value_counts())

print("Age Desc",df['age_desc'].value_counts())

print("Relation",df['relation'].value_counts())

print("ASD",df['Class/ASD'].value_counts())

df['ethnicity'] = df['ethnicity'].replace("?", "White-European")

df['relation'] = df['relation'].replace("?", "Self")

df['age'] = df['age_desc'].replace("?", int(0))

df['age'] = df['age_desc'].astype("int64")

average_age = df['age'].mean()
average_age

#replacing outliers with avg age value
df['age'] = df['age'].replace(0, int(average_age))
df['age'] = df['age'].replace(383, int(average_age))

"""Feature identification with the use of random forest classifier"""

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

X = df.drop(columns=['Class/ASD'])
y = df['Class/ASD']

"""Encoding vategorical features"""

categorical_cols = X.select_dtypes(include='object').columns

for col in categorical_cols:
    encoder = LabelEncoder()
    X[col] = encoder.fit_transform(X[col])

"""Encode target variable"""

encoder = LabelEncoder()
y = encoder.fit_transform(y)
y

df.dropna(axis=0, inplace=True)

#Apply random forest
model = RandomForestClassifier()
model.fit(X, y)

importances = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

N = 21  # Number of top important columns to display

top_columns = feature_importance_df.head(N)['Feature'].tolist()

print("Top", N, "most important columns:")

for column in top_columns:

    print(column)

df = df.drop(['used_app_before', 'age_desc'], axis = 1)
df

display(df.dtypes)

# Value counts of variables
print("\nEthnicity \n",df['ethnicity'].value_counts())

print("\nRelation\n",df['relation'].value_counts())

print("\nAge\n",df['age'].value_counts())

# Bar plot of ASD categories
sns.countplot(x='Class/ASD', data=df)
plt.show()

# Histogram of age distribution
sns.histplot(x='age', data=df)
plt.show()

# Bar plot of gender distribution
sns.countplot(x='gender', data=df)
plt.show()

# Box plot of age distribution by ASD category
sns.boxplot(x='Class/ASD', y='age', data=df)
plt.show()

# Count the number of samples from each ethnicity
df['ethnicity'].value_counts()

# Bar plot of ethnicity distribution
sns.countplot(x='ethnicity', data=df)
plt.xticks(rotation=45)
plt.show()

# Count the different types of relationships
df['relation'].value_counts()

# Bar plot of relationship types
sns.countplot(x='relation', data=df)
plt.xticks(rotation=45)
plt.show()

# Create a countplot of gender distribution by ASD category
sns.countplot(x='Class/ASD', hue='gender', data=df)
plt.show()

# Create a crosstab of relationship type and ASD category
relation_asd_crosstab = pd.crosstab(df['relation'], df['Class/ASD'])

# Display the crosstab
print(relation_asd_crosstab)

# Create a box plot of result by ASD category
sns.boxplot(x='Class/ASD', y='result', data=df)
plt.show()

"""Models that can be used - KNN, Logisitc regression, SVM, Naive bayes

Naive bayes
"""

df.head()

df.drop(columns='age', inplace=True)

df.drop(columns='id', inplace=True)

X = df.drop(columns=['Class/ASD'])
y = df['Class/ASD']

categorical_cols = X.select_dtypes(include='object').columns

for col in categorical_cols:
    encoder = LabelEncoder()
    X[col] = encoder.fit_transform(X[col])

# Encode target variable
encoder = LabelEncoder()
y = encoder.fit_transform(y)
y

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create the GaussianNB model
model = GaussianNB()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

"""Accuracy of naive bayes model"""

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# Generate classification report
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Generate confusion matrix
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

"""Test case naive bayes"""

# columns_to_drop = ['id', 'age_desc', 'used_app_before']

# # Drop the specified columns
# df.drop(columns=columns_to_drop, inplace=True)

# df.drop(columns=['id'], inplace=True)

# Creating a test case for X_train (as provided in the previous response)
test_case = pd.DataFrame({
    'A1_Score': [1, 0, 1, 1, 0],
    'A2_Score': [1, 0, 0, 1, 1],
    'A3_Score': [1, 0, 1, 0, 0],
    'A4_Score': [1, 0, 1, 1, 0],
    'A5_Score': [0, 0, 1, 1, 1],
    'A6_Score': [0, 0, 1, 0, 1],
    'A7_Score': [0, 1, 1, 0, 0],
    'A8_Score': [1, 1, 1, 1, 0],
    'A9_Score': [0, 0, 1, 1, 1],
    'A10_Score': [0, 1, 1, 1, 1],
    'gender': [1, 0, 0, 0, 1],
    'ethnicity': [9, 2, 9, 3, 9],
    'jaundice': [0, 0, 0, 0, 0],
    'autism': [0, 0, 1, 0, 0],
    'country_of_residence': [47, 3, 9, 27, 40],
    'result': [5, 3, 9, 7, 5],
    'relation': [4, 4, 4, 4, 4]
})

# Assuming you have already trained your Naive Bayes model 'nb_model'

# Assuming you have the actual target values for the test case in a separate DataFrame called 'y_test'
y_test_actual = pd.DataFrame({'Class/ASD': [1, 0, 1, 0, 1]})

# Predicting the target variable for the test case
y_pred = model.predict(test_case)

# Calculating the accuracy of the model
accuracy = accuracy_score(y_test_actual, y_pred)

# Printing the accuracy
print("Test Case Accuracy:", accuracy)

"""KNN"""

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Generate classification report
print('Classification Report:')
print(classification_report(y_test, y_pred))

"""test KNN"""

# Creating a test case for X_train (as provided in the previous response)
test_case = pd.DataFrame({
    'A1_Score': [1, 0, 1, 1, 0],
    'A2_Score': [1, 0, 0, 1, 1],
    'A3_Score': [1, 0, 1, 0, 0],
    'A4_Score': [1, 0, 1, 1, 0],
    'A5_Score': [0, 0, 1, 1, 1],
    'A6_Score': [0, 0, 1, 0, 1],
    'A7_Score': [0, 1, 1, 0, 0],
    'A8_Score': [1, 1, 1, 1, 0],
    'A9_Score': [0, 0, 1, 1, 1],
    'A10_Score': [0, 1, 1, 1, 1],
    'gender': [1, 0, 0, 0, 1],
    'ethnicity': [9, 2, 9, 3, 9],
    'jaundice': [0, 0, 0, 0, 0],
    'autism': [0, 0, 1, 0, 0],
    'country_of_residence': [47, 3, 9, 27, 40],
    'result': [5, 3, 9, 7, 5],
    'relation': [4, 4, 4, 4, 4]
})

# Assuming you have already trained your Naive Bayes model 'nb_model'

# Assuming you have the actual target values for the test case in a separate DataFrame called 'y_test'
y_test_actual = pd.DataFrame({'Class/ASD': [1, 0, 1, 0, 1]})

# Predicting the target variable for the test case
y_pred = knn.predict(test_case)

# Calculating the accuracy of the model
accuracy = accuracy_score(y_test_actual, y_pred)

# Printing the accuracy
print("Test Case Accuracy:", accuracy)

"""SVM"""

# Create an instance of SVC classifier
svc = SVC()

# Train the SVC classifier on the training data
svc.fit(X_train, y_train)

# Predict the target values for the test data using SVC
svc_y_pred = svc.predict(X_test)

# Evaluate the accuracy of the SVC classifier
svc_accuracy = accuracy_score(y_test, svc_y_pred)
print("SVM Accuracy:", svc_accuracy)

"""Ridge classifier"""

ridge = RidgeClassifier()

ridge.fit(X_train, y_train)

ridge_y_pred = ridge.predict(X_test)

ridge_accuracy = accuracy_score(y_test, ridge_y_pred)

print("Ridge Accuracy:", ridge_accuracy)

"""Random forest classifier"""

# # Create an instance of Random Forest classifier
rf_classifier = RandomForestClassifier()

# # Train the Random Forest classifier on the training data
rf_classifier.fit(X_train, y_train)

# # Predict the target values for the test data using Random Forest
rf_y_pred = rf_classifier.predict(X_test)

# # Evaluate the accuracy of the Random Forest classifier
rf_accuracy = accuracy_score(y_test, rf_y_pred)
print("Random Forest Accuracy:", rf_accuracy)

"""Decision tree classifier"""

# # Create an instance of Decision Tree Classifier
decision_tree = DecisionTreeClassifier()

# # Train the Decision Tree classifier on the training data
decision_tree.fit(X_train, y_train)

# # Predict the target values for the test data using Decision Tree
dt_y_pred = decision_tree.predict(X_test)

# # Evaluate the accuracy of the Decision Tree classifier
dt_accuracy = accuracy_score(y_test, dt_y_pred)
print("Decision Tree Accuracy:", dt_accuracy)

# # Create an instance of Logistic Regression classifier
logistic_regression = LogisticRegression()

# # Train the Logistic Regression classifier on the training data
logistic_regression.fit(X_train, y_train)

# # Predict the target values for the test data using Logistic Regression
lr_y_pred = logistic_regression.predict(X_test)

# # Evaluate the accuracy of the Logistic Regression classifier
lr_accuracy = accuracy_score(y_test, lr_y_pred)
print("Logistic Regression Accuracy:", lr_accuracy)

print(df.dtypes)

import matplotlib.pyplot as plt

# Count the number of occurrences of each unique value in 'gender' column
gender_counts = df['gender'].value_counts()

# Create a bar plot
plt.bar(gender_counts.index, gender_counts.values)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Distribution of Gender')
plt.show()

# dataset is stored in a DataFrame called "df"
numerical_columns = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                     'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',
                     'result']

numerical_df = df[numerical_columns]

# Calculate correlation matrix
correlation_matrix = numerical_df.corr()

# Create correlation plot
sns.set(style="white")
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

print(X_train.head())

!pip install --upgrade google-colab google-auth

import torch

# Save the model to the Colab's file system
save_path = '/content/my_model.pth'
torch.save(model.state_dict(), save_path)

# Download the model file to your local machine
from google.colab import files
files.download(save_path)

